{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a5f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82a9661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ainao\\\\OneDrive\\\\Project\\\\Fraud detection system\\\\Fraud--Detection---System\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd99a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7234d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ainao\\\\OneDrive\\\\Project\\\\Fraud detection system\\\\Fraud--Detection---System'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6acf7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "      root_dir: Path\n",
    "      train_path: Path\n",
    "      test_path:Path\n",
    "      train_data: Path\n",
    "      test_data:Path\n",
    "      preprocessor: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79506c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"src\"))\n",
    "from fraud_detection.utils.common import read_yaml, create_directories\n",
    "from fraud_detection.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afdac3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "        root_dir = config.root_dir,\n",
    "        train_path = config.train_path,\n",
    "        test_path = config.test_path,\n",
    "        train_data = config.train_data,\n",
    "        test_data = config.test_data,\n",
    "        preprocessor = config.preprocessor\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f092ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from fraud_detection.utils.common import save_object, first_octet \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385bb8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(file_path: Union[str, Path]) -> pd.DataFrame:\n",
    "    return pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: \"DataTransformationConfig\"):\n",
    "        self.config = config\n",
    "\n",
    "    def build_preprocessor(self, df: pd.DataFrame) -> ColumnTransformer:\n",
    "        df_no_target = df.drop(columns=[\"Is Fraudulent\"], errors=\"ignore\")\n",
    "\n",
    "        # Feature type detection\n",
    "        numeric_features = df_no_target.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "        categorical_features = df_no_target.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "        leak_cols = {\"Transaction ID\", \"Customer ID\", \"IP Address\", \"Shipping Address\", \"Billing Address\", \"Transaction Date\"}\n",
    "        categorical_features = [c for c in categorical_features if c not in leak_cols]\n",
    "\n",
    "        transformers = []\n",
    "\n",
    "        # Numeric pipeline\n",
    "        if numeric_features:\n",
    "            num_pipe = Pipeline([(\"scaler\", StandardScaler(with_mean=False))])\n",
    "            transformers.append((\"num\", num_pipe, numeric_features))\n",
    "\n",
    "          # Categorical pipeline\n",
    "        if categorical_features:\n",
    "           try:\n",
    "            cat_encoder = OneHotEncoder(\n",
    "                handle_unknown=\"ignore\",\n",
    "                sparse_output=True, \n",
    "                dtype=np.float32\n",
    "            )\n",
    "           except TypeError:\n",
    "            cat_encoder = OneHotEncoder(\n",
    "                handle_unknown=\"ignore\",\n",
    "                sparse=True,       \n",
    "                dtype=np.float32\n",
    "            )\n",
    "            cat_pipe = Pipeline([(\"encoder\", cat_encoder)])\n",
    "            transformers.append((\"cat\", cat_pipe, categorical_features))\n",
    "\n",
    "        if not transformers:\n",
    "            raise ValueError(\"No numeric or categorical columns available to build a preprocessor.\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "           transformers=transformers,\n",
    "           sparse_threshold=1.0 \n",
    "        )\n",
    "        return preprocessor\n",
    "\n",
    "    @staticmethod\n",
    "    def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "\n",
    "        # datetime features\n",
    "        dt = pd.to_datetime(df[\"Transaction Date\"], errors=\"coerce\")\n",
    "        df[\"tx_year\"] = dt.dt.year\n",
    "        df[\"tx_month\"] = dt.dt.month\n",
    "        df[\"tx_dow\"] = dt.dt.dayofweek\n",
    "        df[\"tx_hour\"] = dt.dt.hour\n",
    "\n",
    "        # amount features\n",
    "        df[\"Is Refund\"] = (df[\"Transaction Amount\"] < 0).astype(int)\n",
    "        df[\"Log Abs Transaction Amount\"] = np.log1p(df[\"Transaction Amount\"].abs())\n",
    "        df[\"Amount Bin\"] = pd.qcut(df[\"Transaction Amount\"], q=4, labels=False, duplicates=\"drop\")\n",
    "\n",
    "        # time-of-day features\n",
    "        hour = df[\"tx_hour\"].where(df[\"tx_hour\"].notna(), df.get(\"Transaction Hour\"))\n",
    "        hour = pd.to_numeric(hour, errors=\"coerce\")\n",
    "        hour = hour.where((hour >= 0) & (hour <= 23))\n",
    "        df[\"Hour Bin\"] = pd.cut(hour, bins=[0, 6, 12, 18, 24], right=False, include_lowest=True, labels=False)\n",
    "\n",
    "        df[\"Day of Week\"] = df[\"tx_dow\"]\n",
    "        df[\"Is Weekend\"] = df[\"Day of Week\"].isin([5, 6]).astype(int)\n",
    "\n",
    "        # address check (normalized)\n",
    "        bill = df.get(\"Billing Address\", pd.Series(index=df.index, dtype=object)).fillna(\"\").astype(str).str.strip().str.lower()\n",
    "        ship = df.get(\"Shipping Address\", pd.Series(index=df.index, dtype=object)).fillna(\"\").astype(str).str.strip().str.lower()\n",
    "        df[\"Address Mismatch\"] = (ship != bill).astype(int)\n",
    "\n",
    "        # customer behaviour\n",
    "        qty = pd.to_numeric(df.get(\"Quantity\"), errors=\"coerce\").replace(0, np.nan)\n",
    "        df[\"Amount per Item\"] = df[\"Transaction Amount\"] / qty\n",
    "        df[\"Age Amount Interaction\"] = df.get(\"Customer Age\", 0) * df[\"Transaction Amount\"]\n",
    "        if \"Account Age Days\" in df.columns:\n",
    "            df[\"Account Age Bin\"] = pd.qcut(df[\"Account Age Days\"], q=4, labels=False, duplicates=\"drop\")\n",
    "\n",
    "        # IP \n",
    "        if \"IP Address\" in df.columns:\n",
    "            df[\"IP First Octet\"] = df[\"IP Address\"].apply(first_octet)\n",
    "\n",
    "        # simple risk flags\n",
    "        q95 = df[\"Transaction Amount\"].quantile(0.95)\n",
    "        df[\"High Value Transaction\"] = (df[\"Transaction Amount\"] > q95).astype(int)\n",
    "        if \"Account Age Days\" in df.columns:\n",
    "            df[\"New Account\"] = (df[\"Account Age Days\"] < 30).astype(int)\n",
    "\n",
    "        # drop leaky/high-cardinality columns\n",
    "        leak_cols = [\"Transaction ID\", \"Customer ID\", \"IP Address\", \"Shipping Address\", \"Billing Address\", \"Transaction Date\"]\n",
    "        df = df.drop(columns=[c for c in leak_cols if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def process_file(self, file_path: Union[str, Path]):\n",
    "        df = load_and_clean_data(file_path)\n",
    "        df = self.engineer_features(df)\n",
    "\n",
    "        # Create aligned X and y\n",
    "        y = df.pop(\"Is Fraudulent\")\n",
    "        X = df\n",
    "        return X, y\n",
    "\n",
    "    def initiate_data_transformation_and_split(self):\n",
    "        # Load and feature engineer\n",
    "        X_train, y_train = self.process_file(self.config.train_path)\n",
    "        X_test,  y_test  = self.process_file(self.config.test_path)\n",
    "\n",
    "        assert len(X_train) == len(y_train), \"Train X/y length mismatch before transform.\"\n",
    "        assert len(X_test)  == len(y_test),  \"Test X/y length mismatch before transform.\"\n",
    "\n",
    "        logging.info(\"Building preprocessing pipeline.\")\n",
    "        preprocessor = self.build_preprocessor(X_train)\n",
    "\n",
    "        logging.info(\"Applying preprocessing pipeline.\")\n",
    "        X_train_processed = preprocessor.fit_transform(X_train)\n",
    "        X_test_processed  = preprocessor.transform(X_test)\n",
    "\n",
    "        assert len(X_train_processed) == len(y_train), \"Train X/y length mismatch after transform.\"\n",
    "        assert len(X_test_processed)  == len(y_test),  \"Test X/y length mismatch after transform.\"\n",
    "\n",
    "        # save the preprocessor\n",
    "        save_object(file_path=self.config.preprocessor, obj=preprocessor)\n",
    "\n",
    "        train_arr = np.c_[X_train_processed,np.array(y_train)]\n",
    "        test_arr = np.c_[X_test_processed,np.array(y_test)]\n",
    "\n",
    "        return train_arr, test_arr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0666366b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-26 10:42:34,977: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-10-26 10:42:34,981: INFO: common: created directory at: artifacts]\n",
      "[2025-10-26 10:42:34,984: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-10-26 10:42:52,294: INFO: 1847238584: Building preprocessing pipeline.]\n",
      "[2025-10-26 10:42:52,601: INFO: 1847238584: Applying preprocessing pipeline.]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.initiate_data_transformation_and_split()\n",
    "except Exception as e:\n",
    "    logging.exception(\"An error occurred during data transformation.\")\n",
    "    raise  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15a15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
